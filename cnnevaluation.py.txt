from keras.preprocessing.image import ImageDataGenerator
from keras.models import model_from_json
import keras.backend as K
import tensorflow as tf
from keras import metrics
import numpy as np
from matplotlib import pyplot

def mean_pred(y_true, y_pred):
    mean =  K.mean(y_pred)
    return mean

def recall_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision


def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))

def specificity_at_sensitivity(**kwargs):
    def metric(labels, predictions):
        # any tensorflow metric
        value, update_op = tf.metrics.true_negatives(labels, predictions, **kwargs)

        # find all variables created for this metric
        metric_vars = [i for i in tf.local_variables() if 'specificity_at_sensitivity' in i.name.split('/')[2]]

        # Add metric variables to GLOBAL_VARIABLES collection.
        # They will be initialized for new session.
        for v in metric_vars:
            tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)

        # force to update metric values
        with tf.control_dependencies([update_op]):
            value = tf.identity(value)
            return value
    return metric

def confusion_matrix(y_true, y_pred):
    y_pred_pos = K.round(K.clip(y_pred, 0, 1))
    y_pred_neg = 1 - y_pred_pos

    y_pos = K.round(K.clip(y_true, 0, 1))
    y_neg = 1 - y_pos

    tp = K.sum(y_pos * y_pred_pos)
    tn = K.sum(y_neg * y_pred_neg)

    fp = K.sum(y_neg * y_pred_pos)
    fn = K.sum(y_pos * y_pred_neg)

    numerator = (tp * tn - fp * fn)
    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

    return numerator / (denominator + K.epsilon())

test_datagen = ImageDataGenerator(rescale=1./255)
test_set = test_datagen.flow_from_directory(
    directory=r'./dataset_7setengseteng/test_set',
    target_size=(100,100),
    color_mode='rgb',
    batch_size=100,
    shuffle=False,
    #seed = 42,
    class_mode='binary'
)

inputdata, labeldata= test_set.next()

#class_dict = test_set.class_indices #{'Parasitized': 0, 'Uninfected': 1}

jsonfile =open('model1.json', 'r')
loadedmodeljson = jsonfile.read()
jsonfile.close()

loadedmodel = model_from_json(loadedmodeljson)
loadedmodel.load_weights('model_nfl.h5')
loadedmodel.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy', 'mae', 'mse', confusion_matrix, recall_m, precision_m, f1_m])

print("input data size : %d" % (len(inputdata)))

#score = loadedmodel.evaluate_generator(generator=test_set)

#history = loadedmodel.fit(inputdata, labeldata, epochs=50, batch_size=len(inputdata), verbose=2)

score = loadedmodel.evaluate(inputdata, labeldata, verbose=1)

# plot metrics
#pyplot.plot(history.history['acc'], label='Acc')
#pyplot.plot(history.history['mean_absolute_error'], label='mae')
#pyplot.plot(history.history['mean_squared_error'], label='mse')
#pyplot.legend()
#pyplot.show()

for i in range(len(score)) :
    print("%s: %.2f%%" % (loadedmodel.metrics_names[i], score[i] * 100))  # output : acc: 78.78%